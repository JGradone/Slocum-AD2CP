{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ca3188e-66b9-4bbe-89ca-82fff4d469c2",
   "metadata": {},
   "source": [
    "# Processing of Slocum Glider-AD2CP Data: RU36 2023\n",
    "\n",
    "jgradone@marine.rutgers.edu     03/10/2022    Initial <br>\n",
    "jgradone@marine.rutgers.edu     06/16/2022    Update for pre-processing <br>\n",
    "engdahl@marine.rutgers.edu      01/19/2024    Modify Notebook 2, 3 & 4 with new example using nbronikowski@mun.ca functions    \n",
    "engdahl@marine.rutgers.edu      04/04/2024    Modified make_dataset to have magnetic declination function to rotate U and V dac\n",
    "\n",
    "**This Jupyter Notebook is intended to:**<br>\n",
    "1) Read glider data frome ERDDAP <br>\n",
    "2) Read in AD2CP data  <br>\n",
    "3) Pre-porcess and Least squares linear inversion on ADCP velocities referenced to true ocean velocity through a depth averaged urrent constraint <br>\n",
    "4) Save output from each segment<br>\n",
    "\n",
    "*Details/comments on what the functions are actually doing in the source code*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e90b6eb5-833a-491f-88f5-4310a8dc3650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import scipy.interpolate as interp\n",
    "from scipy.sparse.linalg import lsqr\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import netCDF4 as nc\n",
    "import math\n",
    "import datetime\n",
    "import xarray as xr\n",
    "import matplotlib.dates as mdates\n",
    "import dask.array as da\n",
    "from erddapy import ERDDAP\n",
    "from netCDF4 import Dataset\n",
    "import gsw\n",
    "import cmocean.cm as cmo\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "## To import functions from Slocum-AD2CP GitHub repository, make this path the path to where the repo exists locally\n",
    "sys.path.insert(0,'../src/analysis/')\n",
    "sys.path.insert(0,'../src/data/')\n",
    "from make_dataset import correct_sound_speed, beam_true_depth, cell_vert, binmap_adcp, beam2enu, inversion, qaqc_pre_coord_transform, qaqc_post_coord_transform, mag_var_correction\n",
    "from analysis import get_erddap_dataset\n",
    "\n",
    "\n",
    "\n",
    "import importlib\n",
    "import NicolaiFunctions\n",
    "importlib.reload(NicolaiFunctions)\n",
    "from NicolaiFunctions import calcAHRS as calcAHRS_NI\n",
    "from NicolaiFunctions import beam2enu as beam2enu_NI\n",
    "from NicolaiFunctions import correct_ad2cp_heading\n",
    "from NicolaiFunctions import inversion as inversion_NI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea15d4cf-ba29-427a-ae12-4f4f8c8a5a2e",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625cd609-b4d7-44b1-9c8f-459c83cf526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset id for erddap call - delayed mode, all glider flight data\n",
    "ds_id = 'ru36-20230525T1748-trajectory-raw-delayed'\n",
    "\n",
    "#dataset id for erddap call - real time mode, has magnetic declination (m_gps_mag_var)\n",
    "ds_id2 = 'ru36-20230525T1748-trajectory-raw-rt'\n",
    "\n",
    "#path to where you have your ad2cp netcdfs\n",
    "path='../Passengers/AD2CP_data_leg2/pass2_0000_netcdf_100000kb_partition_ncs_no_header/'\n",
    "\n",
    "#ad2cp data mode you want to process, Burst or Average *** CASE SENSITIVE ***\n",
    "ad2cp_mode = 'Burst'\n",
    "\n",
    "#inputs to the linear inversion\n",
    "dz=10\n",
    "wDAC = 5\n",
    "wSmoothness = 1\n",
    "\n",
    "#path for output csvs from linear inversion processing\n",
    "csv_path=\"../Passengers/AD2CP_data_leg2/ad2cp_csv/\"\n",
    "\n",
    "# in the steps 3-10 for loop a segment number will be appended to this name\n",
    "csv_name=\"RU36_2023_AD2CP_Processed_Segment_\"\n",
    "\n",
    "#path and filename for the processed netcdf\n",
    "nc_fname='../Passengers/AD2CP_data_leg2/processed_nc/Passengers2_ad2cp_20230525_to_20230614_magvar.nc'\n",
    "\n",
    "#metadata string for the processed netcdf\n",
    "metadata_string=\"RU36 Nortek AD2CP least-squares inversion velocity profile dataset \\nPassengers Project \\nhttp://slocum-data.marine.rutgers.edu/erddap/tabledap/ru36-20230525T1748-trajectory-raw-delayed.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b0db38-51d6-4934-99aa-8fb223dcddab",
   "metadata": {},
   "source": [
    "## Step 1: Load Glider Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f0426-0899-4454-a472-8cf6e180665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load flight data with ds_id\n",
    "variables = ['depth', 'latitude', 'longitude', 'time', 'sci_water_temp', 'sci_water_cond','source_file', 'm_water_vx', 'm_water_vy', 'm_heading']\n",
    "gdf = get_erddap_dataset(ds_id, server='http://slocum-data.marine.rutgers.edu/erddap', variables = variables, filetype='dataframe')\n",
    "gdf.columns = variables\n",
    "\n",
    "## Great way to find start and end times!!\n",
    "start_times = gdf.groupby('source_file').first().time.values\n",
    "end_times   = gdf.groupby('source_file').last().time.values\n",
    "## Remove time zone for slicing ad2cp times\n",
    "start_times2 = pd.to_datetime(start_times).tz_localize(None)\n",
    "end_times2 = pd.to_datetime(end_times).tz_localize(None)\n",
    "\n",
    "\n",
    "\n",
    "## Load flight data with ds_id2, need magnetic declination to rotate depth averaged current later on\n",
    "variables2 = ['time','m_gps_mag_var']\n",
    "gdf2 = get_erddap_dataset(ds_id2, server='http://slocum-data.marine.rutgers.edu/erddap', variables = variables2, filetype='dataframe')\n",
    "gdf2.columns = variables2\n",
    "\n",
    "#merge both delayed and real-time datasets\n",
    "gdf=gdf.merge(gdf2,on='time',how='left')\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b81462-8c79-4f4c-922c-ab6b195b9e7d",
   "metadata": {},
   "source": [
    "## Step 2: Load in AD2CP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6765ccd2-4ced-493c-9f97-6b1ee319a57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = np.sort(glob.glob(path+'*.nc'))\n",
    "\n",
    "tot_ad2cp = xr.open_mfdataset(files,group='Data/'+ad2cp_mode+'/', concat_dim=\"time\", combine=\"nested\")\n",
    "## Because files are not necessarily read in time order with above line\n",
    "tot_ad2cp = tot_ad2cp.sortby('time')\n",
    "## I think this is a documented bug in the xarray open_mfdataset function for grouped NetCDFs\n",
    "config = xr.open_dataset(files[0],group='Config')\n",
    "## So, just assigning the global attributes from the first file to the combined file\n",
    "tot_ad2cp = tot_ad2cp.assign_attrs(config.attrs)\n",
    "tot_ad2cp = tot_ad2cp.rename({'Velocity Range':'VelocityRange','Correlation Range':'CorrelationRange','Amplitude Range':'AmplitudeRange'})\n",
    "# 2 House-keeping steps\n",
    "# 1) Roll shifted 180 for some reason\n",
    "tot_ad2cp['Roll'] = tot_ad2cp['Roll'] \n",
    "# 2) Surface depth is 10 meters. Needed for at least for the 2020 and both 2021 deployments, per conversations with Sven from Nortek.\n",
    "tot_ad2cp['Pressure'] = tot_ad2cp['Pressure']-10\n",
    "# Put time on x-dimension\n",
    "tot_ad2cp = tot_ad2cp.transpose()\n",
    "\n",
    "# !!!! specific to this deployment because it was recovered 6/14 , need to subset time !!!!\n",
    "tot_ad2cp=tot_ad2cp.sel(time=slice('2023-05-25', '2023-06-14'))\n",
    "tot_ad2cp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288d73f4-d42b-45fe-af23-3d40e4bdf73f",
   "metadata": {},
   "source": [
    "## Steps 3-10: Big loop to process velocity data and save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a32573-c17f-4cec-a520-066c3e97a974",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0,len(start_times)):\n",
    "\n",
    "    ## Subset glider df to one segment\n",
    "    subsetgdf = gdf[(gdf.time >= start_times[x]) & (gdf.time <= end_times[x])]\n",
    "    \n",
    "    ind         = np.argwhere(np.isnan(subsetgdf.m_water_vx).ravel()==False).flatten()\n",
    "    \n",
    "    if len(ind)>0:\n",
    "        ## Pull out last non-NaN lat/lon\n",
    "        # ind1         = np.argwhere(np.isnan(subsetgdf.longitude).ravel()==False).flatten()\n",
    "#         vx          = subsetgdf.m_water_vx.iloc[ind1[-1]]\n",
    "#         vy          = subsetgdf.m_water_vy.iloc[ind1[-1]]\n",
    "#         vx_start_lon = subsetgdf.longitude.iloc[ind1[0]]\n",
    "#         vx_start_lat = subsetgdf.latitude.iloc[ind1[0]]\n",
    "#         vx_end_lon   = subsetgdf.longitude.iloc[ind1[-1]]\n",
    "#         vx_end_lat   = subsetgdf.latitude.iloc[ind1[-1]]\n",
    "        \n",
    "        \n",
    "        vx          = subsetgdf.m_water_vx.iloc[ind[-1]]\n",
    "        vy          = subsetgdf.m_water_vy.iloc[ind[-1]]\n",
    "        mag_var     = subsetgdf.m_gps_mag_var.iloc[ind[-1]]\n",
    "        vx_start_lon = subsetgdf.longitude.iloc[ind[0]]\n",
    "        vx_start_lat = subsetgdf.latitude.iloc[ind[0]]\n",
    "        vx_end_lon   = subsetgdf.longitude.iloc[ind[-1]]\n",
    "        vx_end_lat   = subsetgdf.latitude.iloc[ind[-1]]\n",
    "        vx_start_tm = subsetgdf.time.iloc[0]\n",
    "        vx_end_tm   = subsetgdf.time.iloc[-1]\n",
    "        \n",
    "    if subsetgdf.depth.max() < 10:\n",
    "            continue\n",
    "            \n",
    "            \n",
    "    ad2cp_time_ind = np.where((tot_ad2cp.time.values >= start_times2[x]) & (tot_ad2cp.time.values <= end_times2[x]))[0]\n",
    "    if len(ad2cp_time_ind) > 0:\n",
    "\n",
    "        subset_ad2cp = tot_ad2cp.sel(time= slice(tot_ad2cp.time.values[ad2cp_time_ind[0]],tot_ad2cp.time.values[ad2cp_time_ind[-1]]))\n",
    "\n",
    "        # Just check if there is still data after the subsetting \n",
    "        if len(subset_ad2cp.time) > 0:\n",
    "            \n",
    "            # Pre -process data before inversion\n",
    "            subset_ad2cp = correct_sound_speed(subset_ad2cp)\n",
    "            subset_ad2cp = qaqc_pre_coord_transform(subset_ad2cp, corr_threshold = 50, max_amplitude = 75)\n",
    "            subset_ad2cp = beam_true_depth(subset_ad2cp)\n",
    "            subset_ad2cp = binmap_adcp(subset_ad2cp)\n",
    "            # correct heading, vx, and vy with m_gps_mag_var\n",
    "            heading_cor,vx_cor,vy_cor=mag_var_correction(subset_ad2cp['Heading'].values,vx,vy,mag_var)\n",
    "\n",
    "            if ad2cp_mode == 'Average':\n",
    "                subset_ad2cp.attrs['beam2xyz'] = subset_ad2cp.avg_beam2xyz.reshape(4,4)\n",
    "\n",
    "            elif ad2cp_mode=='Burst':  \n",
    "                subset_ad2cp.attrs['beam2xyz'] = subset_ad2cp.burst_beam2xyz.reshape(4,4)\n",
    "            # RotMatrix=calcAHRS_NI(subset_ad2cp['Heading'].values,subset_ad2cp['Roll'].values,subset_ad2cp['Pitch'].values)\n",
    "            RotMatrix=calcAHRS_NI(heading_cor,subset_ad2cp['Roll'].values,subset_ad2cp['Pitch'].values)\n",
    "            # Get your conditions\n",
    "            subset_ad2cp['AHRSRotationMatrix'] = (('x','time'), RotMatrix)\n",
    "            subset_ad2cp = beam2enu_NI(subset_ad2cp)\n",
    "\n",
    "            subset_ad2cp = qaqc_post_coord_transform(subset_ad2cp, high_velocity_threshold=0.75, surface_depth_to_filter = 5)\n",
    "           \n",
    "            # ## Now ready for inversion!\n",
    "\n",
    "            O_ls, G_ls, bin_new, obs_per_bin = inversion_NI(subset_ad2cp.UVelocity.values,subset_ad2cp.VVelocity.values,dz,vx_cor,vy_cor,subset_ad2cp['VelocityRange'].values,subset_ad2cp['Pressure'].values, wDAC, wSmoothness)\n",
    "            now = datetime.now().strftime(\"%m/%d/%y %H:%M:%S\")\n",
    "            print(\"Finished Inversion\", x ,\"out of\",len(start_times),\"at\" ,now)\n",
    "\n",
    "            ###############################################\n",
    "            #             Save master dataset             #\n",
    "            ###############################################\n",
    "            fname = csv_path+csv_name+str(x)+\".csv\".format(dz,x)\n",
    "\n",
    "            ## Make into a dataframe to save as a CSV\n",
    "            d = {'inversion_u': np.real(O_ls), 'inversion_v': np.imag(O_ls), \"inversion_depth\": bin_new,\n",
    "                 \"start_lon\": np.tile(vx_start_lon,len(bin_new)), \"start_lat\": np.tile(vx_start_lat,len(bin_new)),\n",
    "                 \"end_lon\": np.tile(vx_end_lon,len(bin_new)), \"end_lat\": np.tile(vx_end_lat,len(bin_new)),\n",
    "                 \"start_tm\": np.tile(vx_start_tm, len(bin_new)), \"end_tm\": np.tile(vx_end_tm, len(bin_new)),\n",
    "                 \"obs_per_bin\": obs_per_bin}\n",
    "\n",
    "\n",
    "            df = pd.DataFrame(data=d)\n",
    "            df.to_csv(fname,index=False) \n",
    "            now = datetime.now().strftime(\"%m/%d/%y %H:%M:%S\")\n",
    "            print(\"Finished Writing Data\", x ,\"out of\",len(start_times),\"at\" ,now)\n",
    "            del subset_ad2cp\n",
    "            \n",
    "        else:\n",
    "            del subset_ad2cp\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264d322c-1c4b-4518-8068-2d67816ea75e",
   "metadata": {},
   "source": [
    "## Load in Processed AD2CP Data and Export NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472d0a9b-c4f4-46a3-9c78-c9a10522cf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = np.sort(glob.glob(csv_path+'*.csv'))\n",
    "df = pd.concat(map(pd.read_csv, files))\n",
    "\n",
    "inversion_dz = np.diff(df.inversion_depth)[0].astype(int)\n",
    "inversion_depth         = np.arange(np.min(df.inversion_depth),np.max(df.inversion_depth)+inversion_dz,inversion_dz)\n",
    "inversion_time          = np.empty(len(files))\n",
    "inversion_time[:]       = np.NaN\n",
    "inversion_time          = inversion_time.astype(pd.Timestamp)\n",
    "inversion_start_time    = np.empty(len(files))\n",
    "inversion_start_time[:] = np.NaN\n",
    "inversion_start_time    = inversion_time.astype(pd.Timestamp)\n",
    "inversion_end_time      = np.empty(len(files))\n",
    "inversion_end_time[:]   = np.NaN\n",
    "inversion_end_time      = inversion_time.astype(pd.Timestamp)\n",
    "inversion_lat           = np.empty(len(files))\n",
    "inversion_lat[:]        = np.NaN\n",
    "inversion_lon           = np.empty(len(files))\n",
    "inversion_lon[:]        = np.NaN\n",
    "inversion_start_lat     = np.empty(len(files))\n",
    "inversion_start_lat[:]  = np.NaN\n",
    "inversion_start_lon     = np.empty(len(files))\n",
    "inversion_start_lon[:]  = np.NaN\n",
    "inversion_end_lat       = np.empty(len(files))\n",
    "inversion_end_lat[:]    = np.NaN\n",
    "inversion_end_lon       = np.empty(len(files))\n",
    "inversion_end_lon[:]    = np.NaN\n",
    "u_grid = np.empty((len(inversion_depth),len(files)))\n",
    "u_grid[:] = np.NaN\n",
    "v_grid = np.empty((len(inversion_depth),len(files)))\n",
    "v_grid[:] = np.NaN\n",
    "\n",
    "\n",
    "## Loop through by file, load in each file\n",
    "for x in np.arange(0,len(files)):\n",
    "    \n",
    "    df = pd.read_csv(files[x])\n",
    "    \n",
    "    u_grid[np.arange(0,len(df.inversion_u.values)),x] = df.inversion_u.values\n",
    "    v_grid[np.arange(0,len(df.inversion_v.values)),x] = df.inversion_v.values\n",
    "    \n",
    "    inversion_start_time[x] = pd.to_datetime(df.start_tm[0]).tz_localize(None)\n",
    "    inversion_end_time[x] = pd.to_datetime(df.end_tm[0]).tz_localize(None)\n",
    "    mid_time = inversion_end_time[x]-inversion_start_time[x]\n",
    "    \n",
    "    inversion_time[x] = inversion_start_time[x]+mid_time\n",
    "\n",
    "    inversion_start_lat[x] = df.start_lat[0]\n",
    "    inversion_start_lon[x] = df.start_lon[0]\n",
    "    inversion_end_lat[x]   = df.end_lat[0]\n",
    "    inversion_end_lon[x]   = df.end_lon[0]\n",
    "    ## Lat/lon mid point\n",
    "    inversion_lat[x]   = (df.start_lat[0]+df.end_lat[0])/2\n",
    "    inversion_lon[x]   = (df.start_lon[0]+df.end_lon[0])/2\n",
    "\n",
    "\n",
    "    \n",
    "## Now stuff into an organized xarray dataset    \n",
    "ds = xr.Dataset(\n",
    "    data_vars=dict(\n",
    "        u_grid     = ([\"depth\", \"time\"], u_grid),\n",
    "        v_grid     = ([\"depth\", \"time\"], v_grid),\n",
    "        latitude   = ([\"time\"], inversion_lat),\n",
    "        longitude  = ([\"time\"], inversion_lon),\n",
    "        start_lat  = ([\"time\"], inversion_start_lat),\n",
    "        start_lon  = ([\"time\"], inversion_start_lon),\n",
    "        end_lat    = ([\"time\"], inversion_end_lat),\n",
    "        end_lon    = ([\"time\"], inversion_end_lon),\n",
    "        start_time = ([\"time\"], inversion_start_time),\n",
    "        end_time   = ([\"time\"], inversion_end_time)\n",
    "    ),\n",
    "    coords=dict(\n",
    "        time  = inversion_time,\n",
    "        depth = inversion_depth\n",
    "    ),\n",
    "    attrs=dict(description=metadata_string),\n",
    ")\n",
    "\n",
    "## Sort by time because files may not have in read in chronological order\n",
    "ds = ds.sortby(ds.time)\n",
    "#save output to netcdf\n",
    "ds.to_netcdf(nc_fname)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1560daa9-7dad-44b7-8227-707cc208a9ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
